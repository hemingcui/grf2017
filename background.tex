\vspace{-.15in}\section{Research Background} 
\label{sec:background}\vspace{-.075in}

This section first introduces the background of distributed consensus 
(\S\ref{sec:consensus}) and datacenter computing infrastrutures 
(\S\ref{sec:datacenter}). It then presents the motivation of objectives
(\S\ref{sec:motivation}), others' related work (\S\ref{sec:others-work}), and 
the PI's related work (\S\ref{sec:my-work}).

\vspace{-.15in}\subsection{Paxos Consensus} 
\label{sec:consensus}\vspace{-.075in}

Consensus protocols (typically, 
\paxos~\cite{paxos:practical,paxos,paxos:simple,paxos:complex}) play a core
role in datacenters, including 
ordering services~\cite{ellis:thesis,manos:hotdep10,scatter:sosp11},
leader election~\cite{zookeeper, chubby:osdi}, and
fault-tolerance~\cite{eve:osdi12,rex:eurosys14,crane:sosp15}. A \paxos protocol
replicates the same application on a group of computers (or replicas) and 
enforces a strongly consistent order of inputs for this application, as long as 
a majority of replicas still behave correctly. This makes \paxos tolerate 
various faults, including minor replica and network failures.

Due to this strong fault-tolerance, \paxos is widely served in many systems.
For instance, Scatter~\cite{scatter:sosp11} runs 8$\sim$12 replicas in each
\paxos group to order client requests, and it lets replicas respond requests
in parallel. A bigger group size will improve Scatter throughput. 
Moreover, recent state machine replication (SMR) 
systems~\cite{eve:osdi12,rex:eurosys14,crane:sosp15} use \paxos to greatly 
improve the availability of general server applications (\eg, \mysql).

Unfortunately, despite much effort, the group size of traditional \paxos 
protocols can hardly go up to a dozen because their consensus messages 
go through TCP/IP layers (an RTT often takes hundreds of \us), causing the 
consensus latency to increase almost linearly to the group size.

To address this \paxos performance problem, RDMA networking (\eg, 
Infiniband~\cite{infiniband}) becomes a promising direction because its recent 
commonplace in datacenters and and its decreasing prices. An RDMA RTT takes 
only about 3 \us~\cite{pilaf:usenix14}. This ultra low latency not only comes 
from its kernel bypassing feature, but also its dedicated network stack 
implemented in hardware. Therefore, RDMA is considered the fastest kernel 
bypassing technique~\cite{herd:sigcomm14,pilaf:usenix14,dare:hpdc15}; it is 
several times faster than software-only kernel bypassing techniques (\eg, 
DPDK~\cite{dpdk} and Arrakis~\cite{arrakis:osdi14}).

\vspace{-.15in}\subsection{Datacenter Computing Infrastrutures}
\label{sec:datacenter}\vspace{-.075in}



Driven by the drastically increasing computational demands and the volumes of 
data, more and more applications are deployed using two complementary types of 
datacenter infrastrutures: 
schedulers and virtual machines 
(VM). The first type is 
schedulers~\cite{borg:eurosys15,mesos:nsdi11,tupperware, yarn:socc13,
autopilot:sosp07,quincy:sosp09,apollo:osdi14,fuxi:vldb14}. They incorporate 
many critical applications, including trading, 
fraud detection, health care, social-networking, and military applications, and 
these applications naturally require high-availability 
and low performance overhead in deployments. For instance, a high-frequency 
trading application tends to be highly-available during its operation hours, 
and adding merely hundreds of \us to its response time means big money 
lost~\cite{nosql:finance}. Another instance is social-networking: minor machine 
errors in Facebook cluster have led to several whole-site outage 
events~\cite{facebook:outage}.


Existing scheduler infrastrutures only make their own critical components 
highly available, but the applications are not. Therefore, if an application 
crashes or a computational resource goes down (\eg, hardware errors), these 
systems have to reschedule or recompute jobs, leaving an arbitrary unavailable 
time window for these applications.

% 
The second infrastruture type is VM~\cite{amazon:ec2, openstack, vmware}. VM 
has abstract away the heterougous physical computing resources and provide a 
clean, easy to use virtualized resources for applications. Moreover, VM 
deployments provide many benefits such as better resource utilization, energy 
saving. Although some VMs provide primary-backup manner of fault-tolerance, 
this manner has limitations on bandwidth consumption, consistency, etc.

Schedulers and VMs are largely complementary depending on the nature of 
applications such as performance and security. Many schedulers simply use 
partly virtualization techniques such as Docker containers for better 
performance, while many other deployments (\eg, Amazon EC2, OpenStack) simply 
deploy fully virtualized VMs without schedulers or containers, because VMs 
provide better and more complete isolation and security guarantees than 
containers.


\vspace{-.15in}\subsubsection{Motivation of Proposed Objectives} 
\label{sec:motivation}\vspace{-.075in}

Several open problems. First, application is not highly-available. Second, when 
application runs on large number of machines, existing replication systems, 
which mainly target single-machine deployments, can not deploy such 
applications with both properly replication and resource utilization 
considerations. Therefore, we take a holistic approach: first create a fast, 
scable, yet still general consensus protocol (Objective 1), and then we 
integrate this protocol with two major infrastrutures (Objective 2 and 3). This 
novel integratation provides a clean high-availability support for general  
applications.

\vspace{-.15in}\subsection{Related Work by Others} 
\para{Various Consensus Protocols.} \label{sec:others-work}\vspace{-.075in} 
There are a rich set of
\paxos algorithms~\cite{paxos:practical,paxos,paxos:simple,paxos:complex,
epaxos:sosp13} and 
implementations~\cite{paxos:live,paxos:practical,chubby:osdi,crane:sosp15}. 
\paxos is notoriously difficult to be fast and 
scalable~\cite{ellis:thesis,manos:hotdep10,scatter:sosp11}. Since consensus 
protocols play a core role in datacenters~\cite{matei:hotcloud11, mesos:nsdi11, 
datacenter:os} and worldwide 
distributed systems~\cite{spanner:osdi12,mencius:osdi08}, a variety of study 
have been conducted to improve specific aspects of consensus protocols, 
including order commutativity~\cite{epaxos:sosp13}, 
understandability~\cite{raft:usenix14,paxos}, and verifiable reliability 
rules~\cite{modist:nsdi09,demeter:sosp11}.

To make \paxos's throughput scalable (\ie, more replicas, higher throughput), 
various systems leverage \paxos as a core building block to develop advanced 
replication approaches, including partitioning program 
states~\cite{scatter:sosp11,ssmr:dsn14}, splitting consensus 
leadership~\cite{mencius:osdi08,spaxos:srds12}, and hierarchical 
replication~\cite{manos:hotdep10,scatter:sosp11}. Theses approaches have shown 
to largely improve throughput. However, the core of these systems, 
\paxos, still faces an unscalable consensus 
latency~\cite{ellis:thesis,scatter:sosp11,manos:hotdep10}. By using \xxx as a 
building block, these system may scale even better.

\para{Fault-tolerance in datacenter schedulers.} Cluster management 
schedulers~\cite{borg:eurosys15,mesos:nsdi11,tupperware,yarn:socc13,
autopilot:sosp07,quincy:sosp09,apollo:osdi14,fuxi:vldb14} are widespread 
because they can transparently support many diverse applications (\eg, 
Hadoop~\cite{hadoop}, Dryad~\cite{dryad}, and key-value stores~\cite{redis}). 
These existing systems mainly focus on high availability for themselves by 
replicating important components (\eg, controllers) within these systems, or 
focus on fault-recovery of applications~\cite{fuxi:vldb14}. To the best of our 
knowledge, no existing system provides efficient and general high-availability 
service to applications. Although \xxx's current design leverages an existing 
system Mesos~\cite{mesos:nsdi11}, its general \paxos protocol design can also 
be integrated in other systems.

\para{Primary-backup in VM.} TBD.

\para{HPC.} RDMA techniques have been implemented in various 
architectures, including Infiniband~\cite{infiniband}, RoCE~\cite{roce}, and 
iWRAP~\cite{iwrap}. RDMA have been leveraged in many systems to improve 
application-specific latency and throughput, including high performance 
computing~\cite{openmpi}, key-value 
stores~\cite{pilaf:usenix14,herd:sigcomm14,farm:nsdi14,memcached:rdma}, 
transactional processing systems~\cite{drtm:sosp15,farm:sosp15}, and file 
systems~\cite{gibson:nfs}. These systems are largely complementary to \xxx.

\vspace{-.15in}\subsection{Related Work by the PI} 
\label{sec:my-work}\vspace{-.075in}
% 
% First emphasis debugging experience on concurrency. Program analysis.
% Then mention security exploits found in Woodpecker.
% Then mention runtime systems.

The PI is an expert on fault-tolerant distributed systems and datacenter 
computing~\cite{crane:sosp15, tripod:apsys16} and reliable parallel 
runtime systems~\cite{smt:cacm, cui:tern:osdi10, peregrine:sosp11, 
parrot:sosp13}. The PI's works are published in premier conferences on systems 
software (OSDI 2010, SOSP 2011, SOSP 2013, and SOSP 2015) and programming 
languages (PLDI 2012 and ASPLOS 2013). Specifically, the PI has developed a 
general consensus protocol (part of \textbf{Objective 1}) and a preliminary 
fault-tolerant datacenter scheduler (part of \textbf{Objective 2}).




