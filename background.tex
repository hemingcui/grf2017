\vspace{-.15in}\section{Research Background} 
\label{sec:background}\vspace{-.075in}

This proposal involves three main entities: data providers, computation 
providers, and cloud providers. In private clouds, data providers are cloud 
providers; in public clouds, they are different. This section presents 
three most relevant techniques (\S\ref{sec:bigdata}, \S\ref{sec:dft}, and 
\S\ref{sec:sgx}), motivation (\S\ref{sec:motivation}), and 
related work (\S\ref{sec:others-work} and \S\ref{sec:my-work}).


\vspace{-.15in}\subsection{Big-data computing frameworks} 
\label{sec:bigdata}\vspace{-.075in}

Big-data frameworks (\eg, Spark~\cite{nsdi12:spark} and 
MapReduce~\cite{mapreduce}) are popular for computations on tremendous amounts 
of data. These frameworks provide self-defined Java functions (\eg, 
\func{map}/\func{reduce}) to let computation providers write their algorithms, 
and they and automatically apply these functions on the data stored across 
computers in parallel.
% To exchange results across computers, shuffles 
% operations are often invoked and sometimes they are performance bottlenecks in 
% big-data frameworks. For instance, Spark~\cite{nsdi12:spark} has intensive data 
% shuffling beteen its \func{map} and \func{reduce} stages.

% Many-to-many transformations (\eg \func{groupByKey}, \func{join}
% and \func{aggregateByKey}) are prevalent in these frameworks. Each many-to-many
% transformation takes many input records and generates many output records.
% Given an output record, extant data provenance 
% techniques~\cite{icse16:bigdebug,vldb15:titian,vldb16:output}, which track the 
% sequence of transformation operations for data records and can infer the input 
% records on a given output record, will
% report all input records going through this transformation, including many
% irrelevant input records that generate other output records.


To avoid excessive computation, big-data frameworks adopt a lazy 
transformation approach~\cite{pig:vldb08,nsdi12:spark,osdi08:dryad}. Spark 
often uses lazy transformations (\eg, \func{map}), and calls to 
these transformations only create a new data structure called \func{RDD} with 
\emph{lineage} (the sequence of transformations for a data record).
The actual transformations are only triggered when collecting 
operations (\eg, \func{collect}, \func{count}) are called. These collecting 
operations trigger transformations along lineages, so unnecessary 
computations are avoided. \textbf{Objective 1} will leverage lazy 
transformation to create a fast DFT technique called \lazyp (\S\ref{sec:obj1}).

\vspace{-.15in}\subsection{Software-based privacy techniques}
\label{sec:dft}\vspace{-.075in}

Data Flow Tracking (DFT) is a mandatory access control technique for preventing 
sensitive information leakage~\cite{dawn05:taint}. DFT attaches a tag to a 
variable (or object), and this tag will propagate during computations on the 
variable at runtime. DFT has been applied to various areas, such as preventing 
sensitive information (\eg GPS data and contacts) leakage in 
cellphone~\cite{taintdroid:osdi10}, web services~\cite{cloudfence:raid13}, and 
server programs~\cite{libdft:vee12}. To the best of our knowledge, no DFT 
system exists for big-data computing.
 
% Different granularities of computation may incur different levels of 
% computation overhead. Lower level (\eg byte-level) tracking will consume a lot 
% of resources, as each byte of data in an DFT system has its own 
% tags~\cite{libdft:vee12}.  
 
% Multiple research has been focusing on efficiency and applications of DFT.
% Shadowreplica~\cite{shadowreplica:ccs13} proposed to make use of the multicore 
% resources while SHIFT~\cite{hardwardtaint:isca08} suggests accelerating 
% dataflow tracking with hardware support. Several 
% research~\cite{mit07:coverage,fse12:dtam} adopts DFT for providing 
% debugging primitives to improve software reliability.


Complimentary to DFT, statistical techniques, including \func{k}-anonymization 
methods~\cite{kanonymity,icde06:ldiversity} and
differential privacy~\cite{gupt:sigmod12,pinq:sigmod09,airavat:nsdi10}, allow 
the aggregation of sensitive data while adding random noise to preserve 
individual privacy. However, these statistical techniques are either not secure 
(\func{k}-anonymization) or suffering from great losses of accuracy 
(differential privacy). A recent work~\cite{differentialresult:vldb15} reports 
more than 30\% losses of accuracy. For a query results, low accuracy means 
bad utility: a simple KMeans program will return centroids far from the 
accurate ones, and the accuracy loss rate is much larger than the training error 
rate which is several percents in practice.

A key reason for this bad utility problem is that differential privacy can not 
track how sensitive data fields flow to query results, so they have to take a 
coarse-grained approach, which conservatively adds noise to all fields and 
records. \textbf{Objective 2} (\chref{sec:obj2}) proposes a novel fine-grained 
differential privacy technique, which combines  the strengths of DFT and 
differential privacy.
% These 
% statistical techniques  which 
% enforce statistical bounds to prevent individual information leakage.
% Therefore, third-parties can
% not get sensitive data with different queries.



% Overall, despite much effort, existing 
% differential privacy techniques can only favor privacy or utility of results, 
% but not both, and key reason is that these techniques lack a precise tracking 
% of how sensitive data fields flow to query results, so they have to take a 
% coarse-grained approach, which conservatively adds noise to all fields and 
% records.
% 
% 
% In practice, only some fields in a data record may be sensitive and it is too 
% rigorous to make all fields inaccurate. Moreover, different data records may 
% have various security levels. For example, in an Taobao order record 
% $\langle$\v{time}, \v{userId}, \v{productID}$\rangle$, the \v{userId} field is 
% sensitive and it must not be leaked, and different owners of movie rating 
% records may demand different levels of protection for their 
% data. 
% 4. They are not accurate and per-record, only some field are sensitive



\vspace{-.15in}\subsection{Hardware-based privacy techniques}
\label{sec:sgx}\vspace{-.075in}

Trusted Execution Environment (TEE) is a promising technique for
protecting computation in a public cloud even if the cloud's operating systems 
or hypervisors are compromised. For instance, Intel-SGX~\cite{intel-sgx}, a 
popular commercial TEE product, runs a program in a hardware-protected 
\emph{enclave}, so code and data are protected from outside. Compared with the 
approach of computing on encrypted data (\S\ref{sec:others-work}), TEE is much 
safer and 100X to 1000X faster. For instance, a SGX-based system 
Opaque~\cite{opaque:nsdi17} incurs a moderate performance overhead of 30\% 
compared to native big-data queries.

However, to practically run Java big-data queries with SGX, two open challenges 
remain. First, existing SGX-based systems~\cite{opaque:nsdi17} require 
computation providers to manually rewrite the readily pervasive Java 
queries into SGX-compatible C++, a time-consuming and error-prone process.
Second, existing SGX-based systems for big-data have too large Trusted 
Computing Base (TCB). Existing systems 
(\eg, SGX-BigMatrix~\cite{bigmatrix:ccs17}) run a whole language interpreter 
(\eg, JVM and Python runtime) in enclaves, causing a too large (and too 
dangerous) TCB: JVM code comes from many different parties/vendors and 
extremely hard to verified. \textbf{Objective 3} (\chref{sec:obj3}) tackles
these two open challenges by building a new just-in-time compiler.

\vspace{-.15in}\subsection{Motivation of objectives} 
\label{sec:motivation}\vspace{-.075in}

% , and this threat becomes even 
% more pronounced due to more and more data providers, including application 
% vendors individual computer users, are putting their important data on clouds

Data leakage (or breach), defined as the leakage of sensitive customer or 
organization data to unauthorized users~\cite{kazim2015survey}, is a top 
security threat~\cite{top-threats,privacy:bigdata:rand} in cloud computing. In 
a data provider's perspective, both computation providers (\eg, the 2017 iCloud 
account leakage caused by third-parties~\cite{icloud-breach}) and cloud 
providers (\eg, the 2013 Yahoo Cloud compromise~\cite{yahoo-dropbox-breach}) 
have caused severe data leakage and huge financial loss. This proposal aims to 
preserve the data provider's privacy by going two directions. First, we will 
propose two novel complimentary techniques in \textbf{Objective 1} (\kakute) and 
\textbf{Objective 2} (fine-grained differential privacy) to protect privacy 
against the computation providers in private clouds. Second, we will propose 
\textbf{Objective 3} (a new privacy-preserving compiler) to protect privacy 
against the (public) cloud providers. By integrating the outcomes from all three 
objectives, data privacy will be effectively preserved.

\vspace{-.15in}\subsection{Related work by others} 
\label{sec:others-work}\vspace{-.075in}

\para{Computing on encrypted data}. Homomorphic 
encryption~\cite{fullmomo:stoc09} is a
technique for computating on encrypted data in untrusted 
environments. Homomorphic encryption contain two kinds: Fully 
homomorphic encryption (FHE) and partial 
homomorphic encryption.
Partial homomorphic encryption (\eg{} Additive Homomorphic 
Encryption~\cite{paillier})
incurs a much lower overhead compared with FHE. A evaluation~\cite{homo:eval} on
FHE shows a $10e9$ slowdown, which is acceptable in practice.
Systems that adopts PHE (\eg, Monomi~\cite{monomi:vldb13},
Crypsis~\cite{crypsis:hotcloud14}, CryptDB~\cite{cryptdb:sosp11},
MrCrypt~\cite{mrcrypt:oospsla14})
reports a much better overhead, but it has limited expressiveness
(\eg, SQL operators) and requires extra trusted 
servers. Seabed~\cite{seabed:osdi16} proposes asymmetric encryption schemes and 
reduces the performance overhead of AHE, but its expressiveness is still quite
limited.

\para{SGX-based systems}. Intel SGX is a promising technique 
to provide privacy-preserving analytic in public clouds. Compared with 
software-based solutions, hardware-based solutions incurs much lower overhead. 
TrustedDB~\cite{trusteddb:sigmod11} is a hardware-based secure database.
VC3~\cite{vc3:sp15} proposes a secure distributed analytic platform
with read-write validations on MapReduce~\cite{mapreduce}. 
Opaque~\cite{opaque:nsdi17} supports secure and oblivious SQL operators on 
SparkSQL~\cite{sparksql:sigmod15}. However, all these systems have limited 
expressiveness (\eg SQL operators), and VC3 even needs to rewrite the program 
with C++. A recent work~\cite{oblivious:security16} proposes a oblivious 
machine leaning framework on trusted processors. 
SGX-BigMatrix~\cite{bigmatrix:ccs17} proposes an oblivious and secure 
vectorization
abstraction on python, but it has limited expressiveness and it needs to
rewrite the original program with this new abstraction. Although BigMatrix 
provides guideline for writing a oblivious program, but it would be a 
time-consuming and error-prone process.

% Recent work~\cite{securekeeper,opaque:nsdi17} run Zookeeper and SparkSQL in 
% enclaves, and both of them rewrote codes running in enclaves using C++. In the 
% long run, SGX tends not to support Java (partially due to minimizing TCB, see 
% below), thus running unmodified big-data queries with SGX is highly desirable.

\para{Big-data privacy systems}. Big-data privacy has been a top 
emerging threat~\cite{top-threats-nine, kazim2015survey} as more and more user 
sensitive data is stored and processed in clouds. 
Airavat\cite{airavat:nsdi10}, PINQ~\cite{pinq:sigmod09} and 
GUPT~\cite{gupt:sigmod12} propose to apply differential 
privacy~\cite{noise:tcc06} in MapReduce,
to prevent leakage from user query, but differential privacy can result in 
incorrect results. Sedic~\cite{sedic:cloud13} proposes to offload sensitive 
computations to private clouds. MrLazy~\cite{hotcloud14:mrlazy}
proposes a framework of combining data provenance and static DFT analysis for 
self-defined queries, to provide fine-grained information flow for security. 
However, static DFT is not precise and may suffer from false positive. \kakute 
provides fine-grained information control of sensitive data, with no need to 
modify the original program.

% \para{Data provenance}. Given an output record, the data provenance technique 
% can identify inputs that produce this output, so it has shown wide range of 
% applications, including security and 
% debugging~\cite{icse16:bigdebug,dtap:vldb12}.
% RAMP~\cite{ramp:cidr11}, Newt~\cite{newt:socc13}, 
% Pig~\cite{pig:dataflow:pvldb12} and Titian~\cite{vldb15:titian}
% adopt a record-level tracking approach for data provenance in DISC frameworks.
% Chothia~\cite{vldb16:output} introduces a novel framework for output explaining 
% in iterative programs with differential dataflow abstraction. These systems 
% also adopt the record-level tracking approach, so they have low precision in 
% programs containing many-to-many transformations. \kakute, however, provides 
% fine-grained DFT in terms of data field level, so \kakute is much more 
% precise than existing these existing systems on data provenance.


\vspace{-.15in}\subsection{Related work by the PI and co-I} 
\label{sec:my-work}\vspace{-.075in}
% 
% First emphasis debugging experience on concurrency. Program analysis.
% Then mention security exploits found in Woodpecker.
% Then mention runtime systems.

The PI is an expert on secure and reliable distributed 
systems~\cite{smt:cacm, cui:tern:osdi10, peregrine:sosp11,
parrot:sosp13, crane:sosp15, tripod:apsys16, kakute:acsac17, apus:socc17,
confluence:tpds17}. The PI's works are published in top conferences on systems 
(OSDI, SOSP, SOCC, TPDS, and ACSAC) and programming languages (PLDI and ASPLOS). 
Recently, the PI has collaborated with Huawei to launch a technology transfer 
project based on his dependable distributed system~\cite{apus:socc17}.
The co-I is an expert on high-performance 
computing~\cite{powerrock,hwang,jessica,cheung,khokhar}, fault-tolerance~\cite{ 
sheng,shengdi1}, and Java compilers~\cite{rhymes,shengdi,jessica2}. The 
co-I's works are published in top systems conferences (Cluster, SC, 
and ICPADS) and journals (JPDC, TPDS, IEEE Tran. Computers). As 
preliminary results for this proposal, the PI has 
presented \kakute~\cite{kakute:acsac17} in ACSAC '17, and the PI and co-I have 
collaborated to present \confluence~\cite{confluence:tpds17} in TPDS '17.


