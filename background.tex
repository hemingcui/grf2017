\vspace{-.15in}\section{Research Background} 
\label{sec:background}\vspace{-.075in}

This section presents the background of consensus (\S\ref{sec:consensus}) and 
datacenter computing infrastrutures (\S\ref{sec:datacenter}), motivation of 
objectives (\S\ref{sec:motivation}), others' related work 
(\S\ref{sec:others-work}), and PI's related work (\S\ref{sec:my-work}).

\vspace{-.15in}\subsection{Paxos Consensus} 
\label{sec:consensus}\vspace{-.075in}

Consensus protocols (typically, 
\paxos~\cite{paxos:practical,paxos,paxos:simple,paxos:complex}) play a core
role in datacenters, including 
ordering services~\cite{ellis:thesis,manos:hotdep10,scatter:sosp11},
leader election~\cite{zookeeper, chubby:osdi}, and
fault-tolerance~\cite{eve:osdi12,rex:eurosys14,crane:sosp15}. A \paxos protocol
replicates the same application on a group of computers (or \emph{replicas}) 
and enforces the same order of inputs for this application, as long as 
a majority of replicas are still alive. Therefore, \paxos tolerates 
various faults, including minor replica failures and packet losses.

\paxos is widely served in many systems. For instance, 
Scatter~\cite{scatter:sosp11} runs 8$\sim$12 replicas in a
\paxos group to order client requests, and it lets replicas reply requests
in parallel. A bigger group size will improve Scatter throughput. 
Moreover, state machine replication (SMR) 
systems~\cite{eve:osdi12,rex:eurosys14,crane:sosp15} use \paxos to 
improve the availability of server applications (\eg, \mysql~\cite{mysql}).

Unfortunately, the group size of existing \paxos protocols can hardly go up 
to a dozen because their consensus messages go through OS kernels and software 
TCP/IP layers (a round-trip takes about 200 \us), causing the consensus 
latency to increase almost linearly to 
group size~\cite{scatter:sosp11,zookeeper,crane:sosp15}.

To address this \paxos performance problem, Remote Direct Memory Access (RDMA) 
(\eg, Infiniband~\cite{infiniband}) is a promising solution due to its 
commonplace in datacenters and and its decreasing prices. An RDMA round-trip 
takes only about 3 \us~\cite{pilaf:usenix14}. This ultra low latency not only 
comes from its kernel bypassing feature, but also its dedicated network stack 
implemented in hardware. Therefore, RDMA is considered the fastest kernel 
bypassing technique~\cite{herd:sigcomm14,pilaf:usenix14,dare:hpdc15}; it is 
several times faster than software-only kernel bypassing techniques (\eg, 
DPDK~\cite{dpdk} and Arrakis~\cite{arrakis:osdi14}).

\vspace{-.15in}\subsection{Datacenter Computing Infrastrutures}
\label{sec:datacenter}\vspace{-.075in}



To deal with the rapidly increasing volume of data, more and more applications 
are deployed in a datacenter with two complementary types of infrastrutures. The 
first type is schedulers~\cite{borg:eurosys15,mesos:nsdi11,tupperware, 
yarn:socc13, autopilot:sosp07,quincy:sosp09,apollo:osdi14,fuxi:vldb14}. Many 
applications are mission critical and demand both high performance
and availability, including financial trading, health care, social networks, 
and military applications. For instance, a high-frequency trading platform 
must be highly-available during operation hours, and adding merely hundreds of 
\us to its latency means big money lost~\cite{nosql:finance}. Another 
instance is social networks: minor computer errors in Facebook cluster led to 
several outages in recent years~\cite{facebook:outage}.


Although existing schedulers themselves have are made available with 
\paxos (\eg,~\cite{mesos:nsdi11}), their applications are not. Therefore, if an 
application crashes or hardware errors occur, these schedulers have to 
reschedule applications, leading to substantial application unavailability.

% 
The second infrastruture type is VM~\cite{amazon:ec2, openstack, vmware}. VM 
abstracts away the heterougous physical computing resources, making 
computing resources easy to isolate, utilize, and balance loads (\eg, 
via live migration~\cite{vmotion,xen:migration}). Although some application 
fault-tolerance approaches such as primary-backup~\cite{remus:nsdi08}, these 
approaches still often consume prohibitive resources (\eg, CPU and network 
bandwith) and have weaker availability guarantee than \paxos.

Schedulers and VMs are largely complementary infrastrutures depending on 
application requirements such as performance and security. Many schedulers 
use lightweight containers~\cite{docker,lxc} for isolation, while many other 
cloud deployments (\eg, Amazon EC2) simply use VMs without schedulers. 
Therefore, this proposal strenghtens the two infrastrutures respectively.


\vspace{-.15in}\subsection{Motivation of Proposed Objectives} 
\label{sec:motivation}\vspace{-.075in}

% Several open problems. First, application is not highly-available. Second, when 
% application runs on large number of machines, existing replication systems, 
% which mainly target single-machine deployments, can not deploy such 
% applications with both properly replication and resource utilization 
% considerations. Therefore, we take a holistic approach: first create a fast, 
% scable, yet still general consensus protocol (Objective 1), and then we 
% integrate this protocol with two major infrastrutures (Objective 2 and 3). This 
% novel integratation provides a clean high-availability support for general  
% applications.
The proposed objectives stem from two research problems in datacenter 
computing. First, despite the core role and wide deployments of \paxos, it 
suffers from high consensus latency and poor scalability. Therefore, 
\textbf{Objective 1} addresses this problem by leveraging RDMA to create a 
fast, scalable consensus protocol. Second, although many 
datacenter applications demand high availability, existing infrastrutures lack 
such support. To benefit general applications, \textbf{Object 2 and 3} take a 
holistic methodology to integrate our protocol with two major infrastrutures
% , 
% potentially benefiting almost all applications.


\vspace{-.15in}\subsection{Related Work by Others} 
\label{sec:others-work}\vspace{-.075in}

\para{Various Consensus Protocols.}  
There are a rich set of \paxos 
algorithms~\cite{paxos:practical,paxos,paxos:simple,paxos:complex,
epaxos:sosp13} 
and implementations~\cite{paxos:live,paxos:practical,chubby:osdi,crane:sosp15}. 
\paxos is notoriously difficult to be fast and 
scalable~\cite{ellis:thesis,manos:hotdep10,scatter:sosp11}. Since consensus 
protocols play a core role in datacenters~\cite{matei:hotcloud11, mesos:nsdi11, 
datacenter:os} and worldwide 
distributed systems~\cite{spanner:osdi12,mencius:osdi08}, various works 
have been conducted to improve specific aspects of consensus protocols, 
including commutativity~\cite{epaxos:sosp13}, 
understandability~\cite{raft:usenix14,paxos}, and verifiable 
rules~\cite{modist:nsdi09,demeter:sosp11}.

To make \paxos's throughput scalable (\ie, more replicas, higher throughput), 
various systems leverage \paxos as a core building block to develop advanced 
replication approaches, including partitioning program 
states~\cite{scatter:sosp11,ssmr:dsn14}, splitting consensus 
leadership~\cite{mencius:osdi08,spaxos:srds12}, and hierarchical 
replication~\cite{manos:hotdep10,scatter:sosp11}. Theses approaches have shown 
to largely improve throughput. However, the core of these systems, 
\paxos, still faces an unscalable consensus 
latency~\cite{ellis:thesis,scatter:sosp11,manos:hotdep10}. By using \xxx as a 
building block, these prior systems can scale even better.

\para{Fault-tolerance in schedulers.} Datacenter
schedulers~\cite{borg:eurosys15,mesos:nsdi11,tupperware,yarn:socc13,
autopilot:sosp07,quincy:sosp09,apollo:osdi14,fuxi:vldb14} are widespread 
because they can support diverse applications (\eg, 
Hadoop~\cite{hadoop}, Dryad~\cite{dryad}, and key-value stores~\cite{redis}). 
These existing systems mainly focus on high availability for themselves by 
replicating their own important components, or focus on fault-recovery of 
applications~\cite{fuxi:vldb14}. To the best of our knowledge, no existing 
schedulers provide fast and general high-availability service to 
applications.
% \xxx's consensus protocol design is general to application and 
% schedulers so it can be integrated in other systems.

%  Migration and primary-backup.
\para{Fault-tolerance in VM.} Two approaches, primary-backup and live 
migration, exist for improving application reliability in VM. Primary-backup 
uses the hypervisor layer to record application execution state changes in a 
primary VM and frequently propagate the changes to a backup VM on another 
computer. Live migration is invoked for both computer load balance and handling 
failures, and it uses a similar hypervisor technique as primary-backup. Despite 
much clever effort, both these two approaches consume substantial application 
down time (\eg, 8 seconds in vMotion~\cite{vmotion}) and network bandwidth.

% This approach consumes much network bandwidth and has weaker 
% primary-consistency (\eg, if primary rejoins the network) than \paxos.

\para{RDMA techniques.} RDMA has been realized in various types of datacenter 
networks, including Infiniband~\cite{infiniband}, RoCE~\cite{roce}, and 
iWRAP~\cite{iwrap}. RDMA has been leveraged in many software systems to improve 
different performance aspects, including high performance 
computing~\cite{openmpi}, key-value 
stores~\cite{pilaf:usenix14,herd:sigcomm14,farm:nsdi14,memcached:rdma}, 
transactional processing systems~\cite{drtm:sosp15,farm:sosp15}, and file 
systems~\cite{gibson:nfs}. These systems are largely complementary to \xxx.

\vspace{-.15in}\subsection{Related Work by the PI} 
\label{sec:my-work}\vspace{-.075in}
% 
% First emphasis debugging experience on concurrency. Program analysis.
% Then mention security exploits found in Woodpecker.
% Then mention runtime systems.

The PI is an expert on reliable datacenter software systems~\cite{crane:sosp15, 
tripod:apsys16} and reliable, secure multithreading runtime 
systems~\cite{smt:cacm, cui:tern:osdi10, peregrine:sosp11, parrot:sosp13}. The 
PI's works are published in premier conferences on systems software (OSDI 2010, 
SOSP 2011, SOSP 2013, and SOSP 2015) and programming languages (PLDI 2012 and 
ASPLOS 2013). As preliminary results for this \xxx proposal, the PI has 
developed a general consensus protocol~\cite{crane:sosp15} (part of 
\textbf{Objective 1}) and a fault-tolerant datacenter 
scheduler~\cite{tripod:apsys16} (part of \textbf{Objective 2}).




